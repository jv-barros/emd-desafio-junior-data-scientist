# To-Do List

## Following Step by Step
- [x] Gpc - Access
- [x] Big Query - Datario Configuration
- [x] Fork Repository
- [x] Answer `perguntas_sql.md` with SQL
    - [x] `analise_sql.sql` - Create File
- [x] Answer `perguntas_sql.md` with Python and Pandas
    - [x] `analise_python.py` - Create File
- [x] Auth in API
- [x] Answer `perguntas_api.md` with Python and Pandas
    - [x] `analise_api.py` - Create File 
- [x] Visualize API Data
    - [x] `preview_data.py` - Create File
- [x] Create file `to-do-list.md`
- [x] Create file `image_running_scripts.pdf`


`preview_data.py` - Sreamlit view  

## Presentation
   - Explaining steps
       - Explaining access
           - Accessing Google Cloud was straightforward for me as I had already used it in data consumption for automation projects.
       - Explaining answers in `perguntas_sql.md`
           - For the SQL questions, I started by visualizing the tables and identifying the attributes I needed to query. The tables follow a logical pattern for querying and provide the necessary information.
       - Explaining Python script with SQL
           - For the Python queries, I decided to create functions representing each of the questions. Additionally, I embedded the queries within each function. I returned the results with a call to each function.
       - Explaining libraries used
           - I used `basedosdados` for SQL queries with Python. For the API consumption script, I used `requests`, `datetime`, and `statistics`.
       - Explaining API used
           - For API consumption, I closely followed the documentation for each one, which allowed me to query the data for the questions with relative ease.
       - Explaining visualization with Streamlit
           - I chose Streamlit View because it’s a library that offers easy data demonstration and visualization, and it’s also aesthetically pleasing.
